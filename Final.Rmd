---
title: "Final Project"
author: "Evan Eliason"
date: "4/27/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(tibble)
library(ggplot2)
library(plyr)
library(sm)
library(dplyr)
standings_SEIS_seeds <- read_excel("standings_SEIS_seeds.xlsx", 
                                   sheet = "all")
View(standings_SEIS_seeds)

#Create dataframe
standings <- standings_SEIS_seeds
```

# Intro  
## What  
My final project examines the commonalities found in successful teams in the Men's Collegiate Lacrosse
Association (MCLA) since 2012.    

## Why  
This was chosen because of my relationship with the team and the data that the league archives every
year. I have been involved with the team for five years, and thought this would be a great way to marry
a passion of mine with the topics we have covered in SEIS 631.

## How  
As stated earlier, the goal of my project is to determine if there are commonalities between
successful teams in the MCLA. First, A successful team is determined by whether or not they were
selected as a participant in the national tournament at then conclusion of the season. The most
successful team is the National Champion of that respective year. The commonalities evaluated were
each team's offensive strength, defensive strength, overall strength, (strength of schedule), and record overall as well as
in their conference. The metrics to determine these commonalities are as follows: Goals For, Goals
Against, Goal Difference, Wins and Losses, and Conference Wins and Losses.  

# Body
## Obtaining and Cleaning the data
All the data from my project was procured from the mcla.us website. Unfortunately, 
the way they store the data isn't the easiest to pull. I created a python script that
scraped the data from every individual year's final standings page. From here, I had to 
make sure I only took the Division II data, as well as the correct number of teams each
year. Every year there were a different amount of teams, meaning each year required a
slightly different script. Another problem I ran into, was that after 2015, the MCLA 
began including post season stats in the yearly standings, meaning I had to manually
subtract those statistics from the data I scraped. Also, because of Covid
there wasn't complete data for the 2020 season, and the 2021 season only
had a small portion of teams compete. Lastly, I went into excel and manually
added what seeds, and if they eventually won the National Championship.  
Finally, I was able to import my dataset into RStudio. From here I changed all of the
statistics to numeric variables, added a factor variable that was based on if
each team
had a seed, and if they were a National Champion that year into factors as
well. I also created two subsets of my primary dataset, which were the
successful teams and the Champions of the last 10 years.
```{r, echo=FALSE}
#Convert GF and GA to numeric
standings$GF <- as.numeric(as.character(standings$GF))
standings$GA <- as.numeric(as.character(standings$GA))

#Create factor for successful team
standings$success <- factor(ifelse(standings$Seed >0, "Y", "N"))
standings$Champion <- as.factor(as.numeric(standings$Champion))
standings$Year <- as.factor(standings$Year)

ss_stand <- subset(standings, success == "Y")
ss_champions <- filter(ss_stand, as.integer(Champion) == 2)
```
## Initial Visualizations
I began by simply comparing the summaries of the teams who were successful versus not
successful. I immediately saw a couple of large differences in the two groups, so I 
decided to visualize this data to be easily presented.  
Below you can see how the green lines shows that teams who made the tournament
had a much higher average of Goals For than teams that were not successful.  

```{r, echo=FALSE}
success_labels <- factor(standings$success, levels = c(1, 2),
                         labels = c("Not Successful", "Successful"))
sm.density.compare(standings$GF, standings$success, xlab = "Goals For")
title(main = "Density Comparison of 'Goals For' by success")
#legend
colfill <- c(2:(2+length(levels(success_labels))))
legend(250, .009, levels(success_labels), fill=colfill)
```
Unlike the Goal's For density comparison, the Goal's against densities were
relatively similar, with the spread being similar, and only the density around
the average being higher for the Successful teams, versus the Not Successful 
teams. However, another large difference is seen when looking at the summary.
The IQR, and the maximum goals against are both lower for Successful teams, 
which is why the density graph shows a higher peak.

```{r, echo=FALSE}
success_labels_dga <- factor(standings$success, levels = c(1, 2),
                         labels = c("Not Successful", "Successful"))
sm.density.compare(standings$GA, standings$success, xlab = "Goals Against")
title(main = "Density Comparison of 'Goals Against' by success")
#legend
colfill <- c(2:(2+length(levels(success_labels_dga))))
legend(200, .01, levels(success_labels_dga), fill=colfill)
tapply(standings$GA, standings$success, summary) #GA
```
 The Last initial visualization to analyze is where 'Successful' teams are 
 when plotted in regards to Wins and Losses. Below, is the expected result,
 with successful teams, in blue, populating the right side of the graph, and
 the National Champions, in green, in the bottom, right quadrant.  
```{r, echo=FALSE}
standings %>%
  ggplot(aes(x=Wins, y=Losses)) +
  geom_point(alpha = .3) +
  geom_point(data = ss_stand, 
             aes(x=Wins, y=Losses), color = 'blue', size = 2) +
  geom_point(data=ss_champions, 
             aes(x=Wins, y=Losses), color = 'green', size = 2)
```
  
## Distributions  
Next are the distributions of the variables to see if they are Normal. Based 
on the initial visualizations, it is predicted that Goals For will be skewed
to the right, and Goals against will be be more normally distributed in a bell
curve.  
```{r, echo=FALSE, out.width="50%"}
ggplot(standings, aes(x=GF)) + geom_histogram(binwidth = 10)
ggplot(standings, aes(x=GA)) + geom_histogram(binwidth = 10)
```

As expected, the histograms show the predicted result; however, the Goals For
histogram is not as skewed as one may think based on the summary and initial 
observations.  

Next, looking at wins and losses, we see wins are heavily skewed to the right
and losses are uniformly distributed.    
```{r, echo=FALSE, out.width="50%"}
ggplot(standings, aes(x=Wins)) + geom_histogram(binwidth = 1)
ggplot(standings, aes(x=Losses)) + geom_histogram(binwidth = 1)
```
 The last two variables to observe distributions of are Division Wins and 
 Losses. These variables were not previously show in density because of the 
 large disparity in conference strength. These variables are also not the best
 metrics for observing distribution because you are technically only required
 to have 4 conference games each year in order to be eligible for the
 conference and national tournaments. In my observations, most teams only 
 play the minimum amount of conference games, therefore there is not as many
 bins for the data.  
 
```{r, echo=FALSE, out.width="50%"}
ggplot(standings, aes(x=DIVWins)) + geom_histogram(binwidth = 1)
ggplot(standings, aes(x=DIVLosses)) + geom_histogram(binwidth = 1)
```


## Correlation and Regression  
To start off correlation I isolated the numerical variables and looked for any
strong correlations between them. The three strongest positive correlations 
are between Goals For and Wins (.92), Wins and Division Wins (.71), and Goals
Against and Losses (.70). None of these correlations were a surprise, 
especially Wins and Division Wins. The three strongest negative correlations
are between: Wins and Division Losses (-.67), Division Wins and Losses (-.61), and Goals For and Division Losses (-.60). 
```{r, echo=FALSE}
stand_num <- select_if(standings, is.numeric)
cor_stand <- cor(stand_num)
round(cor_stand, 2)
```  

In order to find a linear model to predict champions and seeding I added a
new variable which is numeric value (binary) of champions.I also created a new
data set, of the 2022 teams to predict and test the linear model against. I
then created new datasets consisting only of numeric variables for all my
data, all data except
2022, and only 2022. 

```{r, echo=FALSE}
ss_stand$numChamp <- ifelse(ss_stand$Champion==1, 0, 1)
standings$numChamp <- ifelse(standings$Champion==1, 0, 1)
standings$numSuccess <- ifelse(standings$success=="Y", 1, 0)
stand2022 <- subset(standings, Year == "22")
not22Stand <- subset(standings, Year!= "22")
standings_num <- select_if(standings, is.numeric)
stand2022_num <- select_if(stand2022, is.numeric)
not22Stand_num <- select_if(not22Stand, is.numeric)
standings_num$Column1 <- NULL
standings_num$numChamp <- NULL
standings_num$playoffs <- NULL
standings_num$Seed <- NULL

```

I will be looking to first find three linear models to predict if a team makes
it to the national tournament (success), then if a team will likely be the
National Champion without knowing the seed, and then predicting from the top
16 teams, which team will be the National Champion  
### Regression to predict if a team goes to National tournament  
The first regression model is determined using the AIC forward method. The 
final variables that are used are Wins, DIVWins, and Goals Against.
```{r, echo=FALSE}
min.playoff_pred <- lm(formula = numSuccess ~ 1, data = standings_num)
max.playoff_pred <- lm(formula = numSuccess ~ ., data = standings_num)
summary(min.playoff_pred)
fwd.playoff_pred <- step(min.playoff_pred, direction = 'forward', 
                         scope = formula(max.playoff_pred))
plot(fwd.playoff_pred$anova$AIC, type = 'l')
```
```{r}
success_pred <- lm(numSuccess ~ Wins, DIVWins + GA, data = standings_num)
summary(success_pred)
```
The model 'success_pred', a model to determine if a team makes the playoffs, 
has an F-statistic of 246.3 meaning that there is a relationship between the
predictor and response variables. However R squared is only .4495, which is 
not very close to 1 meaning this model isn't a great fit, but is the best fit available.

```{r, echo=FALSE}
test_2022 <- stand2022
pred_play_22 <- predict.lm(success_pred, newdata = stand2022_num)
test_2022$pred_play <- as.numeric(pred_play_22)
test_2022 <- test_2022[with(test_2022,order(-pred_play)),]
test_2022[1:16,c(2,10,12,15)]
```
This regression model correctly predicted 14 out of the 16 teams that got 
invited to the national tournament this year, by ranking the teams by the 
value of pred_play_22. This was better than I expected, because teams get an
automatic bid for winning their conference, so underdog's sometimes could win,
but not be a top 16 team statistically.

### Predicting Champions
I went about creating a prediction model for finding a champion given no 
playoff knowledge. This model had much different results than the previous.
The best F-statistic I was able to get was 15.41 and the highest R-squared was
.0524. I believe the reasoning for this is because of how few champions there
were in the data (only 8 in the training data). I had hypothesized that this
would be an easier metric than predicting the playoff's because of underdogs
receiving an automatic bid for winning their conference. However, as seen 
in the summary and plots below, this model doesn't predict very well.
```{r, echo=FALSE}
not22Stand_num$Column1 <- NULL
not22Stand_num$numChamp <- ifelse(not22Stand_num$numChamp==1, 0,1)
not22Stand_num$Seed <- NULL
not22Stand_num$playoffs <- NULL
not22Stand_num$numSuccess <- NULL
noPlayoffChamp <- lm(numChamp ~ GF+ GA + Wins + 
                       Losses, data = not22Stand_num)
```

```{r, echo=FALSE, out.width="50%"}
summary(noPlayoffChamp)
plot(noPlayoffChamp)
```
I decided to test my model and see what the top 5 most likely champions were predicted to be. The top five most likely champions were all in the tournament, and the eventual winner had the 3rd highest value from the model.
```{r, echo=FALSE}
noPlayoffChamp_22 <- predict.lm(noPlayoffChamp, newdata = stand2022_num)
test_2022$noPlayChamp <- as.numeric(noPlayoffChamp_22)
test_2022 <- test_2022[with(test_2022,order(-noPlayChamp)),]
test_2022[1:5,c(2,10,12,16)]
```
### Regression model of Champions Given Seeds
```{r, echo=FALSE}
df_not22 <- not22Stand
df_not22$Column1 <- NULL
df_not22$Champion <- NULL
df_not22$success <- NULL
df_not22 <- subset(df_not22, Year!="22")
df_not22$Year <- NULL
df_not22$TEAM <- NULL
min.champPred <- lm(formula = numChamp ~ 1, data = df_not22)
max.champPred <- lm(formula = numChamp ~ ., data = df_not22)
summary(min.champPred)
fwd.champPred <- step(min.champPred, direction = 'forward', 
                         scope = formula(max.champPred))
plot(fwd.champPred$anova$AIC, type = 'l')
```
The final variables chosen were the binary variable numSuccess, which tells
whether or not a team made the playoffs, their playoff Seed, Division Wins,
Goals For, and Wins. This generated a R-squared of .1951, and an F-statistic
of 40.39. Neither of these metrics show theres a great deal of fit; however, this again may be because of the low amount of National Champions.
```{r}
champPred <- lm(numChamp ~ numSuccess + Seed + DIVWins + GF + Wins, data = df_not22)
summary(champPred)
```
```{r, echo=FALSE}
champPred_22 <- predict.lm(champPred, newdata = stand2022_num)
test_2022$championPred <- as.numeric(champPred_22)
test_2022 <- test_2022[with(test_2022,order(-championPred)),]
test_2022[1:10,c(2,13,15,17)]
```

# Topics From Class

## Topic 1: R Markdown  
R Markdown was used to make the method used, metrics gathered, and analyzed results easier to read
in a common format.

## Topic 2: Github
Github was used to publish findings and how the final results were determined.

## Topic 3: Distribution and ggplot
GGPlot will be used to show distributions of statistics and frequencies of teams
selected to the national tournament

## Topic 4: Linear Regression and Correlation
Linear Regression was used to find try and predict if a team would be selected to
make it to the national tournament and be considered a "successful team". This
was also used to predict which team, from the top 16 would become then National
Champion.

## Topic 5: Probability
Probability was used to make and show predictions of results based on the data available. Probability was also used in distribution histograms. Showing if you score a certain amount of points, or gather enough wins, you have a higher probability of making the tournament. 
This will also be tested against the real results to show accuracy, train algorithms, and provide evidence 
for theories.

# Conclusion
Before this project, I expected to find the Goals Against, Goals For, and Wins
were the most important variables when predicting success. I also believed 
that I would be able to create pretty reliable and accurate models for
predicting future National Champions. However, on a couple of fronts I refuted
my hypotheses. First of all, I found that it was easier to accurately predict
the teams that would be admitted to the tournament. Using Wins, Division wins
and Goals against I created a model with a strong fit that correctly predicted
14/16 teams in the tournament. Looking at my first visuals, this makes more
sense. There is a large drop off from playoff team's stats, versus teams that didn't make it. I was surprised at my last model, creating a regression model
that included the playoff seeds team's had. Since 2012, only one time did a 
seed other than 1 or 2 win, so having such a low fit seemed odd; however, it
the model was correct in not having confidence as a 6 seed won this year. In
Conclusion, I learned a great deal about regression, probability, and 
visualizing data with ggplot.